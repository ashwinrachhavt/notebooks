{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langmem import create_manage_memory_tool, create_search_memory_tool\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_community.tools import DuckDuckGoSearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = InMemoryStore(\n",
    "    index = {\n",
    "        \"dims\": 1536,\n",
    "        \"embed\": \"openai:text-embedding-3-small\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In December, we kicked off the agentic era by releasing an experimental version of Gemini 2.0 Flash — our highly efficient workhorse model for developers with low latency and enhanced performance. Earlier this year, we updated 2.0 Flash Thinking Experimental in Google AI Studio, which improved its performance by combining Flash's speed with the ability to reason through more complex problems. Today's releases mark a new chapter for our Gemini model. With the release of Gemini 2.0 Flash, and the series of research prototypes exploring agentic possibilities, we have reached an exciting milestone in the Gemini era. And we're looking forward to continuing to safely explore all the new possibilities within reach as we build towards AGI. The new model from Google DeepMind has the catchy name Gemini-Exp-1114 and has matched the latest version of GPT-4o and exceeded the capabilities of the o1-preview reasoning model from OpenAI. The latest Gemini 2.0 AI model lineup from Google, offers something for everyone, but the key lies in understanding what each model brings to the table. Are you looking for speed and simplicity? Google launched its much-anticipated new flagship AI model, Gemini 2.0 Pro Experimental, on Wednesday. The announcement was part of a series of other AI model releases. The company is also making ..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "response = search.invoke(\"Latest model from Gemini\")\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "snippet: In this recording, I speak about langmem. This is one of the newest innovation from langchain. It focuses on long term memory. I believe we should focus on m..., title: Langmem | Long term memory from Langchain - YouTube, link: https://m.youtube.com/watch?v=7QU89qL795A, snippet: The LangMem SDK has been introduced as an open-source library designed to enhance long-term memory capabilities for AI agents. This new software aims to enable various agent functionalities by implementing different memory types, allowing agents to learn from their interactions. Developers are encouraged to explore the SDK, which can be ..., title: LangMem SDK Launched as Open-Source Library for AI ... | DeepNewz, link: https://deepnewz.com/software/langmem-sdk-launched-open-source-library-ai-agents-long-term-memory-capabilities-f20d29d1, snippet: 今天LangChain发布了 LangMem SDK，这是一个可帮助你的代理通过长期记忆进行学习和改进的库。它提供了从对话中提取信息、通过提示更新来优化代理行为，以及维护关于行为、事实和事件的长期记忆的工具。, title: LangChain 团队发布 LangMem SDK | 宝玉的分享, link: https://baoyu.io/translations/langchain-langmem-sdk, snippet: Today, we are excited to announce the first steps towards long-term memory support in LangGraph, available both in Python and JavaScript.Long-term memory lets you store and recall information between conversations so your agent can learn from feedback and adapt to user preferences.This feature is part of the OSS library, and it is enabled by default for all LangGraph Cloud & Studio users., title: Launching Long-Term Memory Support in LangGraph, link: https://blog.langchain.dev/launching-long-term-memory-support-in-langgraph/"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "search = DuckDuckGoSearchResults()\n",
    "display(Markdown(search.invoke(\"Langmem\", output_format=\"json\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_react_agent(\n",
    "    \"gpt-4o-mini\",\n",
    "    tools = [\n",
    "        create_manage_memory_tool(namespace=(\"memories\",)),\n",
    "        create_search_memory_tool(namespace=(\"memories\"))\n",
    "    ],\n",
    "    store = store\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your lighting preference is for dark mode.\n"
     ]
    }
   ],
   "source": [
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Remember that I prefer dark mode.\"}]}\n",
    ")\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What are my lighting preferences?\"}]}\n",
    ")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
